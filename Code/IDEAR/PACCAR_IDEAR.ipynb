{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Data Miner\n",
    "## ITD Advanced Analytics\n",
    "- Date: 2018/05\n",
    "- Credit: Based on MS TDSP Utilities\n",
    "\n",
    "This notebook steps through the most common analyses performed in the early stages of the [CRISP DM](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining) analysis process. The notebook is specifically centered around \"Data Understanding\" and covers basic \"Data Preparation\" tasks as well. \n",
    "\n",
    "More advanced data preparation and modeling work is use-case-specific and is not captured in this generic workflow. Rather, this notebook is intended to serve as a launchpad for more detailed analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML('''<style>\n",
    "    .widget-label { min-width: 20ex !important; }\n",
    "    .widget-text { min-width: 60ex !important; }\n",
    "</style>'''))\n",
    "\n",
    "#Toggle Code\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    "\n",
    " } else {\n",
    " $('div.input').show();\n",
    "\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);//commenting code disabling by default\n",
    "</script>\n",
    "<form action = \"javascript:code_toggle()\"><input type=\"submit\" value=\"Toggle Raw Code\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------\n",
    "\n",
    "# <a name=\"setup\"></a> 1.0 Global Configuration and Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import scipy.stats as stats\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from functools import partial\n",
    "import IPython\n",
    "import ipywidgets\n",
    "from ipywidgets import widgets\n",
    "from ipywidgets import interact, interactive,fixed\n",
    "import operator\n",
    "from IPython.display import Javascript, display,HTML, clear_output\n",
    "from ipywidgets import widgets, VBox, Output, Layout\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "import yaml\n",
    "import copy\n",
    "import warnings\n",
    "# import getpass\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from helpers import *\n",
    " \n",
    "%matplotlib inline\n",
    "font={'family':'normal','weight':'normal','size':8}\n",
    "matplotlib.rc('font',**font)\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 5.0)\n",
    "matplotlib.rc('xtick', labelsize=9) \n",
    "matplotlib.rc('ytick', labelsize=9)\n",
    "matplotlib.rc('axes', labelsize=10)\n",
    "matplotlib.rc('axes', titlesize=10)\n",
    "sns.set_style('whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Specify and load yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "#Read in yaml file as text\n",
    "if (sys.version_info > (3, 0)):    \n",
    "    yaml_file_name = input('Please enter yaml file Name: ')\n",
    "else:\n",
    "    yaml_file_name = raw_input('Please enter yaml file Name: ')\n",
    "\n",
    "Sample_Size = 10000\n",
    "\n",
    "# %%add_conf_code_to_report\n",
    "\n",
    "conf_dict = ConfUtility.parse_yaml(yaml_file_name)\n",
    "\n",
    "# Read in data from local file or SQL server\n",
    "if 'DataSource' not in conf_dict:\n",
    "    df=pd.read_csv(conf_dict['DataFilePath'], skipinitialspace=True)\n",
    "\n",
    "# Making sure that we are not reading any extra column\n",
    "df = df[[each for each in df.columns if 'Unnamed' not in each]]\n",
    "\n",
    "# Filter out unnecessary columns\n",
    "necessary_columns = conf_dict['CategoricalColumns'] + conf_dict['NumericalColumns'] + [conf_dict['Target']]\n",
    "df = df[list(set(necessary_columns))]\n",
    "\n",
    "# Save a backup of df and numerical columns, pre-manipulation\n",
    "df_original = df\n",
    "conf_dict['OriginalNumericalColumns'] = copy.deepcopy(conf_dict['NumericalColumns'])\n",
    "conf_dict['OriginalCategoricalColumns'] = copy.deepcopy(conf_dict['CategoricalColumns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "# 2.0 Prepare Data\n",
    "Section 2 covers typical data manipulation steps to prepare data for analysis. It is common to first run the full notebook, then evaluate features to identify outliers, non-normal distributions, etc. that are then remediated by coming back to section 2 for cleansing. \n",
    "\n",
    "After applying changes in any subsection of this section, be sure to hit the \"Apply\" buttons to re-compile the notebook with the changes you've made.\n",
    "\n",
    "## 2.1 Filtering\n",
    "Filters are applied in this first section such that any transformations of the data are reflected in analyses below. Note that changes made to the data in this section are not automatically propagated through the document, subsequent cells must be re-compiled individually with Ctrl+Enter or by selecting *Cell > Run All Below* (recommended)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tool supports filtering on ranges. For more refined filtering methods (e.g. validating values against other columns), custom code can be added by expanding the source code and adding user-defined functions. Any custom function must produce a Pandas DataFrame object named 'df' to propagate through the notebook.\n",
    "\n",
    "[[Hit \"Toggle Raw Code\" button at top of notebook to edit user-defined filters]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate missingness\n",
    "# NA values will crash the model fits, so let's see how to handle them\n",
    "\n",
    "## Function to count NAs\n",
    "def count_NA(data):\n",
    "    na_counts = data.isnull().sum(axis=0).reset_index(name='NACount')\n",
    "    ## Print out the list of columns with missing values\n",
    "    return(na_counts[na_counts.NACount>0].sort_values('NACount', ascending=False))\n",
    "\n",
    "def FillNACols(data, colname, value):\n",
    "    data[colname][data[colname].isna()] = value\n",
    "    return(data)\n",
    "\n",
    "# Inspect NA counts\n",
    "## Get the counts of NAs\n",
    "na_counts = count_NA(df)\n",
    "## Print details about each column with NAs\n",
    "for idx in na_counts['index']:\n",
    "    print('Feature: {}'.format(idx))\n",
    "    print('Number of NAs: {}'.format(na_counts.NACount[na_counts['index']==idx].item()))\n",
    "    print(df[idx].unique())\n",
    "    print('==============================================')\n",
    "\n",
    "# Fill values\n",
    "## Simple fills\n",
    "df = FillNACols(df, 'Alley', 'None')\n",
    "df = FillNACols(df, 'PoolQC', 'None')\n",
    "df = FillNACols(df, 'MiscFeature', 'None')\n",
    "df = FillNACols(df, 'Fence', 'None')\n",
    "df = FillNACols(df, 'FireplaceQu', 'None')\n",
    "df = FillNACols(df, 'GarageFinish', 'None')\n",
    "df = FillNACols(df, 'GarageCond', 'None')\n",
    "df = FillNACols(df, 'GarageQual', 'None')\n",
    "df = FillNACols(df, 'GarageType', 'None')\n",
    "df = FillNACols(df, 'BsmtExposure', 'None')\n",
    "df = FillNACols(df, 'BsmtQual', 'None')\n",
    "df = FillNACols(df, 'BsmtCond', 'None')\n",
    "df = FillNACols(df, 'BsmtFinType1', 'None')\n",
    "df = FillNACols(df, 'BsmtFinType2', 'None')\n",
    "\n",
    "## Remove Rows\n",
    "df = df = df[pd.notna(df['MasVnrType'])]\n",
    "df = df = df[pd.notna(df['Electrical'])]\n",
    "\n",
    "## Remove Columns\n",
    "cols_to_drop = ['GarageYrBlt', 'LotFrontage']\n",
    "try:\n",
    "    df.drop(columns=cols_to_drop, inplace=True)\n",
    "except:\n",
    "    print(\"Columns already dropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Update conf_dict to reflect the new df\n",
    "conf_dict['CategoricalColumns'] = list(set(conf_dict['CategoricalColumns']) - set(cols_to_drop))\n",
    "conf_dict['NumericalColumns'] = list(set(conf_dict['NumericalColumns']) - set(cols_to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run all cells below\n",
    "def run_all(ev):\n",
    "    display(Javascript('IPython.notebook.execute_cells_below()'))\n",
    "\n",
    "print(\"Confirm Filter Application\")\n",
    "button_rerun = widgets.Button(description=\"Apply\")\n",
    "button_rerun.on_click(run_all)\n",
    "display(button_rerun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Log Transformations\n",
    "It may be necesarry to transform certain columns. In particular, it is common to log-transform numerical columns to ensure a normal distribution. If a feature fails the normality tests below, try creating a log-transformed version of that feature here- this version may pass a normality test.\n",
    "\n",
    "Note: When a column is log-transformed, the original column (e.g. \"Age\") is replaced by the transformed version (\"Age_log\"). If you decide not to use the transformation, simply re-run this section with your column de-selected to restore the original version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset values\n",
    "df_xform = copy.deepcopy(df)\n",
    "# conf_dict['NumericalColumns'] = copy.deepcopy(conf_dict['OriginalNumericalColumns'])\n",
    "log_transforms = {}\n",
    "\n",
    "# Define a multiple-select list to select columns for transformation\n",
    "log_list = widgets.SelectMultiple(\n",
    "    options=conf_dict['NumericalColumns'],\n",
    "    description='Select Column(s)',\n",
    "    disabled=False,\n",
    "    layout=Layout(display=\"flex\", flex_flow='column')\n",
    ")\n",
    "\n",
    "display(log_list)\n",
    "\n",
    "# And a button to execute the transform\n",
    "out_log = Output()\n",
    "with out_log:\n",
    "        clear_output()\n",
    "\n",
    "def apply_log(ev):\n",
    "    for feature in log_list.value:\n",
    "        # Create new feature\n",
    "        new_feature = feature + '_log'\n",
    "        df_xform[new_feature] = np.log1p(df_xform[feature])\n",
    "        conf_dict['NumericalColumns'].append(new_feature)\n",
    "        # Drop untransformed feature\n",
    "#         conf_dict['NumericalColumns'].remove(feature)\n",
    "#         df_xform.drop([feature], axis = 1, inplace = True)\n",
    "        log_transforms.update({new_feature: {'type': 'log1p', 'source_feature': feature}})\n",
    "        with out_log: \n",
    "            print(\"Generated log transform of \" + feature + \": \" + new_feature)\n",
    "            print(\"Removed \" + feature + \" from numerical features\")\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "button_log = widgets.Button(description=\"Add Transforms\")\n",
    "button_log.on_click(apply_log)\n",
    "display(button_log)\n",
    "\n",
    "display(out_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Apply log transforms to notebook\")\n",
    "display(button_rerun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Discretization\n",
    "In some cases, it may be beneficial to bin numerical values. This can be achieved in the following section. Binning will add columns with an appended *_bin* label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binned = copy.deepcopy(df_xform)\n",
    "conf_dict['CategoricalColumns'] = copy.deepcopy(conf_dict['OriginalCategoricalColumns'])\n",
    "\n",
    "# Prep a clean-sheet output to reflect transforms being applied\n",
    "out_disc = Output()\n",
    "with out_disc:\n",
    "        clear_output()\n",
    "\n",
    "# Define a multiple-select list to select columns for transformation\n",
    "feature_list = widgets.Dropdown(\n",
    "    options=conf_dict['NumericalColumns'],\n",
    "    description='Feature:'\n",
    ")\n",
    "\n",
    "bincount = widgets.IntSlider(\n",
    "    min=2, max=100,\n",
    "    step=1, value=4,\n",
    "    description='Number of Bins')\n",
    "\n",
    "display(feature_list)\n",
    "display(bincount)\n",
    "\n",
    "\n",
    "\n",
    "# Pre-allocate dict to log transformations\n",
    "bin_transforms = {}\n",
    "\n",
    "# And a button to execute the transform\n",
    "def apply_bin(ev):\n",
    "    # Create new feature\n",
    "    feature = feature_list.value\n",
    "    new_feature = feature + '_bin'\n",
    "    # Create the binned feature\n",
    "    df_binned[new_feature] = pd.qcut(df_binned[feature], bincount.value, duplicates = 'drop')\n",
    "    # Add the feature \n",
    "    conf_dict['CategoricalColumns'].append(new_feature)\n",
    "    with out_disc: \n",
    "        print(\"Generated binned version of \" + feature + \": \" + new_feature)\n",
    "    # Update the log\n",
    "    bin_transforms.update({new_feature: {'bin_count': bincount.value, 'source_feature': feature}})\n",
    "            \n",
    "\n",
    "\n",
    "button_bin = widgets.Button(description=\"Apply Discretization\")\n",
    "button_bin.on_click(apply_bin)\n",
    "display(button_bin)\n",
    "\n",
    "display(out_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Apply binning to notebook\")\n",
    "display(button_rerun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Export Transformations\n",
    "\n",
    "The transformations specified in the sub-sections of section 2 are automatically exported to a YAML file for reproducibility. See \"transformation_logs.yaml\" in your working directory for a record of transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = {}\n",
    "           \n",
    "            \n",
    "# Get log transforms\n",
    "for i in log_transforms:\n",
    "    if i in transforms:\n",
    "        transforms[i].update({'log': log_transforms[i]})\n",
    "    else:\n",
    "        transforms.update({i:{'log': log_transforms[i]}})\n",
    "\n",
    "\n",
    "\n",
    "# Get binning transforms\n",
    "for i in bin_transforms:\n",
    "    if i in transforms:\n",
    "        transforms[i].update({'discretize': bin_transforms[i]})\n",
    "    else:\n",
    "        transforms.update({i:{'discretize': bin_transforms[i]}})\n",
    "\n",
    "\n",
    "# Sort by keys\n",
    "transforms = dict(collections.OrderedDict(sorted(transforms.items())))\n",
    "\n",
    "# Save transformations to yaml\n",
    "with open('transformation_logs.yaml', 'w') as outfile:\n",
    "    yaml.dump(transforms, outfile, default_flow_style=False)\n",
    "    \n",
    "print(\"Exported transformation log to ./transformation_logs.yaml\")\n",
    "print(\"\\n=========================== \\nTransformation Log: \")\n",
    "print(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "\n",
    "# 3.0 Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sampling Data if data size is larger than 10k\n",
    "df0 = df_binned # df0 is the unsampled data. Will be used in data exploration and analysis where sampling is not needed\n",
    "         # However, keep in mind that your final report will always be based on the sampled data. \n",
    "    \n",
    "\n",
    "if Sample_Size < df_binned.shape[0]:\n",
    "    df = df_binned.sample(Sample_Size)\n",
    "else:\n",
    "    df = df_binned\n",
    "\n",
    "# change float data types\n",
    "if 'FloatDataTypes' in conf_dict:   \n",
    "    for col_name in conf_dict['FloatDataTypes']:\n",
    "        df[col_name] = df[col_name].astype(float)      \n",
    "        \n",
    "# Getting the list of categorical columns if it was not there in the yaml file\n",
    "if 'CategoricalColumns' not in conf_dict:\n",
    "    conf_dict['CategoricalColumns'] = list(set(list(df.select_dtypes(exclude=[np.number]).columns)))\n",
    "\n",
    "# Getting the list of numerical columns if it was not there in the yaml file\n",
    "if 'NumericalColumns' not in conf_dict:\n",
    "    conf_dict['NumericalColumns'] = list(df.select_dtypes(include=[np.number]).columns)    \n",
    "\n",
    "# Exclude columns that we do not need\n",
    "if 'ColumnsToExclude' in conf_dict:\n",
    "    conf_dict['CategoricalColumns'] = list(set(conf_dict['CategoricalColumns'])-set(conf_dict['ColumnsToExclude']))\n",
    "    conf_dict['NumericalColumns'] = list(set(conf_dict['NumericalColumns'])-set(conf_dict['ColumnsToExclude']))\n",
    "\n",
    "# Ordering the categorical variables according to the number of unique categories\n",
    "filtered_cat_columns = []\n",
    "temp_dict = {}\n",
    "for cat_var in conf_dict['CategoricalColumns']:\n",
    "    temp_dict[cat_var] = len(np.unique(df[cat_var].dropna()))\n",
    "sorted_x = sorted(temp_dict.items(), key=operator.itemgetter(0), reverse=True)\n",
    "conf_dict['CategoricalColumns'] = [x for (x,y) in sorted_x]\n",
    "\n",
    "ConfUtility.dict_to_htmllist(conf_dict,['Target','CategoricalColumns','NumericalColumns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.2 Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print ('The data has {} Rows and {} columns'.format(df0.shape[0],df0.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.3 Print the column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"The types of columns are:\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  3.4 Extract Descriptive Statistics of Each Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def num_missing(x):\n",
    "    return len(x.index)-x.count()\n",
    "\n",
    "def num_unique(x):\n",
    "    return len(np.unique(x.dropna()))\n",
    "\n",
    "\n",
    "temp_df = df0.describe().T\n",
    "\n",
    "sum_df = pd.DataFrame(df0.sum(numeric_only = True))\n",
    "sum_df.columns = ['sum']\n",
    "\n",
    "missing_df = pd.DataFrame(df0.apply(num_missing, axis=0)) \n",
    "missing_df.columns = ['missing']\n",
    "\n",
    "unq_df = pd.DataFrame(df0.apply(num_unique, axis=0))\n",
    "unq_df.columns = ['unique']\n",
    "\n",
    "types_df = pd.DataFrame(df0.dtypes)\n",
    "types_df.columns = ['DataType']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.4.1 Numerical Columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summary_df = temp_df.join(sum_df).join(missing_df).join(unq_df).join(types_df).drop_duplicates()\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.4.2 Categorical Columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "col_names = list(types_df.index)\n",
    "num_cols = len(col_names)\n",
    "index = range(num_cols)\n",
    "cat_index = []\n",
    "for i in index:\n",
    "    if col_names[i] in conf_dict['CategoricalColumns']:\n",
    "        cat_index.append(i)\n",
    "summary_df_cat = missing_df.join(unq_df).join(types_df.iloc[cat_index], how='inner') #Only summarize categorical columns\n",
    "summary_df_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----------------------------------------------\n",
    "\n",
    "# 4.0 Explore Individual Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Explore the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def histPlot(data, target):\n",
    "    plt.hist(data[target])\n",
    "    plt.show()\n",
    "\n",
    "interact(histPlot,\n",
    "        data=fixed(df),\n",
    "        target=widgets.Dropdown(\n",
    "            options=conf_dict['NumericalColumns'],\n",
    "            value=conf_dict['Target'],\n",
    "            description='Target:',\n",
    "            disabled=False,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.2 Numeric Variable Summaries & Normality Tests\n",
    "The following plots provide summary visualizations for numerical features. \n",
    "\n",
    "Along the top row of charts, we see a histogram and density plot of the distribution. \n",
    "\n",
    "The bottom-left chart shows a QQ-Plot: For a normal distribution, the blue points will generally fall on the red diagonal line. In addition, a normality test is run and results are printed at the top (p<0.05 = not normal, p>=0.05 is normal).\n",
    "\n",
    "Finally, the bottom-right chart shows a boxplot of the feature. Note that black circles are statistical outliers, it may be worth considering filtering these values in the filtering section at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w1_value = ''\n",
    "w1 = None\n",
    "w1 = widgets.Dropdown(\n",
    "        options=conf_dict['NumericalColumns'],\n",
    "        value=conf_dict['NumericalColumns'][0],\n",
    "        description='Numeric Variable:',\n",
    "    )\n",
    "\n",
    "interact(NumericAnalytics.custom_barplot, df=fixed(df), col1=w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.3 Categorical Variable Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w1_value = ''\n",
    "w1 = None\n",
    "\n",
    "w1 = widgets.Dropdown(\n",
    "    options = conf_dict['CategoricalColumns'],\n",
    "    value = conf_dict['CategoricalColumns'][0],\n",
    "    description = 'Categorical Variable:',\n",
    ")\n",
    "\n",
    "interact(CategoricAnalytics.custom_barplot, df=fixed(df), col1=w1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  4.4 Interactions Between Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.4.1 Rank variables based on linear relationships with reference variable\n",
    "In this section, we review the relationships between numeric and categorical variables against a reference variable (likely your target). This is a quick way to understand which features may contribute to the predictive value of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols_list = [conf_dict['Target']] + conf_dict['NumericalColumns'] + conf_dict['CategoricalColumns'] \n",
    "cols_list = list(OrderedDict.fromkeys(cols_list)) \n",
    "w1 = widgets.Dropdown(    \n",
    "    options=cols_list,\n",
    "    value=cols_list[0],\n",
    "    description='Ref Var:'\n",
    ")\n",
    "w2 = ipywidgets.Text(value=\"5\", description='Top Num Vars:')\n",
    "w3 = ipywidgets.Text(value=\"5\", description='Top Cat Vars:')\n",
    "\n",
    "interact(InteractionAnalytics.rank_associations, df=fixed(df),conf_dict=fixed(conf_dict), col1=w1, col2=w2, col3=w3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  4.4.2 Interactions between categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w1, w2 = None, None\n",
    "\n",
    "if conf_dict['Target'] in conf_dict['CategoricalColumns']:\n",
    "    cols_list = [conf_dict['Target']] + conf_dict['CategoricalColumns'] \n",
    "    cols_list = list(OrderedDict.fromkeys(cols_list)) \n",
    "else:\n",
    "    cols_list = conf_dict['CategoricalColumns']\n",
    "    \n",
    "w1 = widgets.Dropdown(\n",
    "    options=cols_list,\n",
    "    value=cols_list[0],\n",
    "    description='Categorical Var 1:'\n",
    ")\n",
    "w2 = widgets.Dropdown(\n",
    "    options=cols_list,\n",
    "    value=cols_list[1],\n",
    "    description='Categorical Var 2:'\n",
    ")\n",
    "\n",
    "interact(InteractionAnalytics.categorical_relations, df=fixed(df), col1=w1, col2=w2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  4.4.4 Interactions between numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w1, w2 = None, None\n",
    "\n",
    "if conf_dict['Target'] in conf_dict['NumericalColumns']:\n",
    "    cols_list = [conf_dict['Target']] + conf_dict['NumericalColumns'] \n",
    "    cols_list = list(OrderedDict.fromkeys(cols_list)) \n",
    "else:\n",
    "    cols_list = conf_dict['NumericalColumns']\n",
    "w1 = widgets.Dropdown(\n",
    "    options=cols_list,\n",
    "    value=cols_list[0],\n",
    "    description='Numerical Var 1:'\n",
    ")\n",
    "w2 = widgets.Dropdown(\n",
    "    options=cols_list,\n",
    "    value=cols_list[1],\n",
    "    description='Numerical Var 2:'\n",
    ")\n",
    "\n",
    "interact(InteractionAnalytics.numerical_relations, df=fixed(df), col1=w1, col2=w2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.4.5 Correlation matrix between numerical variables\n",
    "The correlation matrix represents the results of mass correlation analyses between all numerical variables. A value of 1 represents a perfect correlation, while 0 indicates no correlation. \n",
    "\n",
    "The matrix can use multiple correlation methods. Some guidelines for selecting the correct method are below. For continuous features we have:\n",
    "\n",
    "* Pearson: Most widely used method, measures degree of association. Variables must be linear and normally distributed and must be continuous (e.g. age, weight).\n",
    "\n",
    "When incorporating rank data, use one of the following:\n",
    "\n",
    "* Spearman: Similar to Pearson, but no assumption of normal distribution and the variables can be either continuous or ordinal (e.g. finishing position in a race, rank in class). This is the more popular rank metric over Kendall.\n",
    "* Kendall: More robust than Spearman when working with small datasets, or datasets with errors. This robustness comes with a tradeoff on performance, with Kendall being exponentially slower for large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import quiver, colorbar, clim,  matshow\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "df2 = df[conf_dict['NumericalColumns']].corr(method='kendall')\n",
    "col_names = list(df[conf_dict['NumericalColumns']].columns)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(15,15))\n",
    "m = ax.matshow(df2, cmap=matplotlib.pyplot.cm.coolwarm)\n",
    "ax.grid(b=False)\n",
    "fig.colorbar(m)\n",
    "ax.set_xticklabels([' '] + col_names, rotation=90)\n",
    "ax.set_yticklabels([' '] + col_names)\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  4.4.6 Interactions between numerical and categorical variables\n",
    "In this section we perform hypothesis tests to determine whether the values of a categorical variable (e.g. your target) have statistically different distributions. Note that a p-value is calculated to reflect one of two interpretations:\n",
    "\n",
    "* p<=0.05: We reject the null hypothesis that the different levels of the categorical variable are equal. In other words, the levels are significantly different. \n",
    "* p>0.05: Accept the null hypothesis that the levels of the categorical feature are equal. There is no difference in the value of the numerical feature between our categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w1, w2 = None, None\n",
    "\n",
    "if conf_dict['Target'] in conf_dict['NumericalColumns']:\n",
    "    cols_list = [conf_dict['Target']] + conf_dict['NumericalColumns'] #Make target the default reference variable\n",
    "    cols_list = list(OrderedDict.fromkeys(cols_list)) #remove variables that might be duplicates with target\n",
    "else:\n",
    "    cols_list = conf_dict['NumericalColumns']\n",
    "    \n",
    "w1 = widgets.Dropdown(\n",
    "    options=cols_list,\n",
    "    value=cols_list[0],\n",
    "    description='Numerical Variable:'\n",
    ")\n",
    "\n",
    "if conf_dict['Target'] in conf_dict['CategoricalColumns']:\n",
    "    cols_list = [conf_dict['Target']] + conf_dict['CategoricalColumns'] #Make target the default reference variable\n",
    "    cols_list = list(OrderedDict.fromkeys(cols_list)) #remove variables that might be duplicates with target\n",
    "else:\n",
    "    cols_list = conf_dict['CategoricalColumns']\n",
    "    \n",
    "w2 = widgets.Dropdown(\n",
    "    options=cols_list,\n",
    "    value=cols_list[0],\n",
    "    description='Categorical Variable:'\n",
    ")\n",
    "interact(InteractionAnalytics.nc_relation, df=fixed(df),conf_dict=fixed(conf_dict), col1=w1, col2=w2, col3=fixed(w3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.4.8 Interactions between two numerical and one categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w1, w2, w3 = None, None, None\n",
    "\n",
    "if conf_dict['Target'] in conf_dict['NumericalColumns']:\n",
    "    cols_list = [conf_dict['Target']] + conf_dict['NumericalColumns'] \n",
    "    cols_list = list(OrderedDict.fromkeys(cols_list)) \n",
    "else:\n",
    "    cols_list = conf_dict['NumericalColumns']\n",
    "    \n",
    "w1 = widgets.Dropdown(\n",
    "    options = cols_list,\n",
    "    value = cols_list[0],\n",
    "    description = 'Numerical Var 1:'\n",
    ")\n",
    "w2 = widgets.Dropdown(\n",
    "    options = cols_list,\n",
    "    value = cols_list[1],\n",
    "    description = 'Numerical Var 2:'\n",
    ")\n",
    "\n",
    "if conf_dict['Target'] in conf_dict['CategoricalColumns']:\n",
    "    cols_list = [conf_dict['Target']] + conf_dict['CategoricalColumns'] \n",
    "    cols_list = list(OrderedDict.fromkeys(cols_list)) \n",
    "else:\n",
    "    cols_list = conf_dict['CategoricalColumns']\n",
    "    \n",
    "w3 = widgets.Dropdown(\n",
    "    options = cols_list,\n",
    "    value = cols_list[0],\n",
    "    description = 'Legend Cat Var:'\n",
    ")\n",
    "interact(InteractionAnalytics.nnc_relation, df=fixed(df),conf_dict=fixed(conf_dict), col1=w1, col2=w2, col3=w3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---------------------------------\n",
    "# 5.0 Latent Trait Analysis & Dimension Reduction\n",
    "\n",
    "## 5.1 Principal Components Analysis (PCA)\n",
    "Principal component analysis re-calculates *n* principal components for a dataset with *n* numerical features. The principal components are derived sequentially to capture the largest amount of variation in the data, i.e. PC1 captures the most variation, followed by PC2, then PC3... Each principal component is a combination of the original features. \n",
    "\n",
    "Practically speaking, PCA is commonly used for two purposes: Dimension reduction and latent trait analysis. For the purposes of this workflow, we are interested in latent trait analysis. \n",
    "\n",
    "PCAs can often capture latent traits in our data in the same way an IQ test estimates intelligence (the latent trait) from questions (our numerical features). The variable factor compass shows how the original numerical features influence the \"momentum\" of points in our PCA scatterplot. For example, if we saw an \"oil temperature\" arrow pointing north-east on our compass, we would ascertain that points in the top right of our scatter plot have high oil temperatures, while points in the bottom left have lower oil temp. Furthermore, arrows that are close to eachother imply a relationship between those values. If our \"oil temperature\" vector points in a similar direction to \"ambient temperature\", a possible relationship between those values is implied. If the scatterplot shows many points flagged as \"failures\" in the top right, this points to a relationship between ambient temp, oil temp and failure.\n",
    "\n",
    "The final chart shows the percentage of variation captured by each principal component. When drawing insights from PCA, it's important to make sure that these insights are based on a sufficient portion of the variation. For example, if PC1 captures 40% of variation and PC2 captures 35%, a scatterplot of PC1 and PC2 explains 75% of the variation and we can draw fairly robust conclusions. Conversely, if PC1 and PC2 only capture 5% of variation, conclusions are fairly disconnected from the complete dataset.\n",
    "\n",
    "----------------------------------\n",
    "Interesting relationships exist between 2-4, 2-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def numerical_pca_egv(df, conf_dict, col1, col2, col3):\n",
    "    # Do the PCA.\n",
    "    n_components = len(conf_dict['NumericalColumns'])\n",
    "    df2 = df[conf_dict['NumericalColumns']]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df2)\n",
    "    df2 = scaler.transform(df2)\n",
    "    df2 = pd.DataFrame(df2, columns = conf_dict['NumericalColumns'])\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced = pca.fit_transform(df2)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "\n",
    "    # Append the principle components for each entry to the dataframe\n",
    "    for i in range(0, n_components):\n",
    "        df2['PC' + str(i + 1)] = reduced[:, i]\n",
    "\n",
    "    #display(df2.head())\n",
    "    if col1 not in conf_dict['NumericalColumns']:\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df2.reset_index(drop=True, inplace=True)\n",
    "        df2[col1] = df[col1]\n",
    "\n",
    "    # Show the points in terms of the first two PCs\n",
    "    g = sns.lmplot(('PC' + str(col2)),\n",
    "                   ('PC' + str(col3)),\n",
    "                   hue=col1,\n",
    "                   data=df2,\n",
    "                   fit_reg=False,\n",
    "                   scatter=True,\n",
    "                   size=7)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot a variable factor map for the first two dimensions.\n",
    "    (fig, ax) = plt.subplots(figsize=(8, 8))\n",
    "    for i in range(0, len(pca.components_)):\n",
    "        ax.arrow(0,\n",
    "                 0,  # Start the arrow at the origin\n",
    "                 pca.components_[int(col2) - 1, i],  #0 for PC1\n",
    "                 pca.components_[int(col3) - 1, i],  #1 for PC2\n",
    "                 head_width=0.05,\n",
    "                 head_length=0.08)\n",
    "\n",
    "        plt.text(pca.components_[int(col2) - 1, i] + 0.05,\n",
    "                 pca.components_[int(col3) - 1, i] + 0.05,\n",
    "                 df2.columns.values[i])\n",
    "\n",
    "    an = np.linspace(0, 2 * np.pi, 100)\n",
    "    plt.plot(np.cos(an), np.sin(an))  # Add a unit circle for scale\n",
    "    plt.axis('equal')\n",
    "    ax.set_title('Variable factor compass')\n",
    "    plt.show()\n",
    "\n",
    "    # Do a scree plot\n",
    "    ind = np.arange(0, n_components)\n",
    "    cumpca = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    (fig, ax) = plt.subplots(figsize=(8, 6))\n",
    "    sns.pointplot(x=ind, y=cumpca)\n",
    "    ax.set_title('Scree plot')\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticklabels(ind)\n",
    "    ax.set_xlabel('Component Number')\n",
    "    ax.set_ylabel('Explained Variance')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "num_numeric = len(conf_dict['NumericalColumns'])\n",
    "if  num_numeric > 3:\n",
    "    \n",
    "    w1, w2, w3 = None, None, None\n",
    "    if conf_dict['Target'] in conf_dict['CategoricalColumns']:\n",
    "        cols_list = [conf_dict['Target']] + conf_dict['CategoricalColumns'] \n",
    "        cols_list = list(OrderedDict.fromkeys(cols_list)) \n",
    "    else:\n",
    "        cols_list = conf_dict['CategoricalColumns']\n",
    "\n",
    "    \n",
    "    w1 = widgets.Dropdown(\n",
    "        options = cols_list,\n",
    "        value = cols_list[0],\n",
    "        description = 'Legend Variable:',\n",
    "        width = 10\n",
    "    )\n",
    "    w2 = widgets.Dropdown(\n",
    "        options = [str(x) for x in np.arange(1,num_numeric+1)],\n",
    "        value = '1',\n",
    "        width = 1,\n",
    "        description='PC at X-Axis:'\n",
    "    )\n",
    "    w3 = widgets.Dropdown(\n",
    "        options = [str(x) for x in np.arange(1,num_numeric+1)],\n",
    "        value = '2',\n",
    "        description = 'PC at Y-Axis:'\n",
    "    )\n",
    "    \n",
    "    interact(numerical_pca_egv, \\\n",
    "             df=fixed(df), \\\n",
    "             conf_dict= fixed(conf_dict), \\\n",
    "             col1=w1, col2=w2, col3=w3)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {
    "04f0d91d56f342708c52e57fbe9bcd06": {
     "views": [
      {
       "cell_index": 45
      }
     ]
    },
    "0506975d69c14717be17695d1a6c7132": {
     "views": [
      {
       "cell_index": 50
      }
     ]
    },
    "0dc41da3c3f445d690100fd99558f244": {
     "views": [
      {
       "cell_index": 30
      }
     ]
    },
    "2196ee8000864f23bbfa4689bfaffc7d": {
     "views": [
      {
       "cell_index": 37
      }
     ]
    },
    "4a5ca2989b4d456d8d019f38514c3485": {
     "views": [
      {
       "cell_index": 35
      }
     ]
    },
    "56d3942cb1684869808846d861e7b0b9": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "642af708746142059091fb9c21fb4612": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "9cc891c4a4dc4ca3bc4801bf7c38e19d": {
     "views": [
      {
       "cell_index": 41
      }
     ]
    },
    "a416687c67684df8818f966704d9f073": {
     "views": [
      {
       "cell_index": 43
      }
     ]
    },
    "d0c17a981a554bcc89cba56b4a1edccd": {
     "views": [
      {
       "cell_index": 39
      }
     ]
    },
    "d0d887fc32c940e88721b75f39ec03c5": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    },
    "e96f8216a4b94c3aa524bce1034708ef": {
     "views": [
      {
       "cell_index": 32
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
