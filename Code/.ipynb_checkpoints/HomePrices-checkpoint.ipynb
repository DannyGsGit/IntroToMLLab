{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PillowLogo.jpg\" width=\"400\">\n",
    "\n",
    "# Hands-On Lab: Home Price Estimation\n",
    "\n",
    "## Introduction\n",
    "In this lab, PACCAR is entering the real estate market and taking on Zillow! We will be estimating home prices, showing how good data and good science get us the best recommendations. Specifically, we'll walk through:\n",
    "* A simple linear regression\n",
    "* AutoML\n",
    "* Data understanding & feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import any packages we will need to use for the analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import yaml\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import power_transform\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "When working in code, we first need to import our data. This can come from databases, spreadsheets, websites, etc.\n",
    "\n",
    "```\n",
    "[!] It is important to remember that data is loaded into your computer's memory. When people talk about \"Big Data\", it is commonly describing datasets that don't fit into memory!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our dataset and investigate the first several rows\n",
    "home_data = pd.read_csv(\"../Data/Train.csv\")\n",
    "home_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should start by getting familiar with the dataset. Some common questions (and answers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is our target variable?\n",
    "print(\"SalePrice\")\n",
    "home_data.SalePrice.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows of data do we have?\n",
    "print('Number of rows: {}'.format(len(home_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many attributes do we have for each row of data? What are the features?\n",
    "print('Number of columns: {}'.format(home_data.shape[1]))\n",
    "print(list(home_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, the city of Ames, Iowa does a terrific job of cataloging data. Have a look at *./Documentation/data_description.txt* for more information about what these inputs mean. Unfortunately, this is a rare treat but a state we should strive for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What data types are the features?\n",
    "home_data.columns.to_series().groupby(home_data.dtypes).groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Quality\n",
    "A strong suite of validation measures is critical to the success of an ML deployment.\n",
    "\n",
    "For home pricing, we will use RMSE- Root Mean Square Error. \n",
    "<img src=\"rmse.jpg\" width=\"400\">\n",
    "RMSE is commonly used in regression. We often communicate MAE (Mean Absolute Error), as it is more intuitive, but RMSE generally makes a better measure for maximizing model performance. The reason why is in the square term. RMSE penalizes larger errors more severely than smaller errors. \n",
    "\n",
    "For home pricing, we apply a small variation on RMSE, which is to measure the difference in the Logs of the prices. This is to assign equal weight to lower and higher priced homes.\n",
    "\n",
    "```\n",
    "[!] Plot Twist: This dataset comes from a public competition, for which there is a leaderboard! Throughout the lab, we will compare our performance against the leaderboard.\n",
    "```\n",
    "\n",
    "Our goal is to medal. Let's see what it takes!\n",
    "<img src=\"medals.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RMSE formula to be used later\n",
    "def getRMSE(data, target, prediction):\n",
    "    data['num'] = (np.log(data[prediction]) - np.log(data[target]))**2\n",
    "    RMSE = np.sqrt(sum(data.num) / len(data))\n",
    "    RMSE\n",
    "    return(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next chunk imports the leaderboard. Inspecting the scores shows 0.06626 is the score to beat, with most scores significantly under 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import leaderboard\n",
    "leaderboard = pd.read_csv(\"../Data/house-prices-advanced-regression-techniques-publicleaderboard.csv\")\n",
    "\n",
    "# Sort by score\n",
    "leaderboard = leaderboard.sort_values('Score')\n",
    "leaderboard.Score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine distribution of scores in first quartile\n",
    "leaderboard[leaderboard.Score < 0.3].Score.hist(bins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data for Modeling\n",
    "When building predictive models, it is crucial that we split our data into *train* and *test* sets. This keeps our models honest as they are trained on one set of data, then tested on another. Later, we will talk about related concepts of overfitting and target leakage in relationship to these splits.\n",
    "\n",
    "<img src=\"train_test_split.svg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data columns into features and target tables\n",
    "X = home_data.drop(columns=['SalePrice', 'Id'])\n",
    "y = home_data[['SalePrice']].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then split the rows into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1984)\n",
    "\n",
    "print(\"Training Rows: {} \\nTest Rows: {}\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "## Linear Model\n",
    "Our first approach will be a linear regression fit of home prices. This is similar to the type of fit we would perform in Minitab. Linear models are an excellent place to start due to their interpretability- we know exactly how predictions are being generated. \n",
    "\n",
    "On the other hand, linear models assume linear fits... an assumption that must be tested! Below we see an example of Anscombe's Quartet. From Wikipedia: \n",
    "*The data sets in the Anscombe's quartet are designed to have approximately the same linear regression line (as well as nearly identical means, standard deviations, and correlations) but are graphically very different. This illustrates the pitfalls of relying solely on a fitted model to understand the relationship between variables.*\n",
    "\n",
    "Only the top-left quartet would be considered an appropriate model! \n",
    "\n",
    "<img src=\"AnscombesQuarter.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode any categorical variables\n",
    "def one_hot_mixed_df(data):\n",
    "    data_cat = data.select_dtypes(include='object')\n",
    "    data_num = data.select_dtypes(exclude='object')\n",
    "    ## One-hot transform the objects\n",
    "    data_cat_dummies = pd.get_dummies(data_cat,drop_first=True)\n",
    "    ## Join dummies with numerics\n",
    "    data_lm = pd.concat([data_cat_dummies, data_num], axis=1, sort=False)\n",
    "    return(data_lm)\n",
    "\n",
    "# Apply transformations\n",
    "X_lm = one_hot_mixed_df(X)\n",
    "\n",
    "## Verify results\n",
    "print('Total Columns: {}'.format(X.shape[1]))\n",
    "print('Total Columns post-Dummy: {}'.format(X_lm.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape prior to NA cleanup: {}\".format(X_lm.shape))\n",
    "\n",
    "# NA values will crash the model fits, so let's see how to handle them\n",
    "def count_NA(data):\n",
    "    na_counts = data.isnull().sum(axis=0).reset_index(name='NACount')\n",
    "    ## Print out the list of columns with missing values\n",
    "    return(na_counts[na_counts.NACount>0])\n",
    "    \n",
    "print(count_NA(X_lm))\n",
    "\n",
    "## We'll make a simplifying assumption that we can drop lot frontage and garage year built features. \n",
    "X_lm.drop(columns=['LotFrontage', 'GarageYrBlt'], inplace=True)\n",
    "\n",
    "## For MasVnrArea, we will drop the NA rows. Note that rows also need to be dropped from our target!!!\n",
    "row_drop_index =X_lm.MasVnrArea.notna()\n",
    "X_lm = X_lm[row_drop_index]\n",
    "y_lm = y[row_drop_index]\n",
    "\n",
    "print(\"Shape of train after NA cleanup: {}\".format(X_lm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then split the rows into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lm, y_lm, test_size=0.3, random_state=1984)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally fit the linear model to an object called \"reg\", then look at some information about the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear model\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# Print summaries\n",
    "print(\"Score (Training):\")\n",
    "print(reg.score(X_train, y_train))\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Score (Test):\")\n",
    "print(reg.score(X_test, y_test))\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Intercept:\")\n",
    "print(reg.intercept_)\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Coefficients:\")\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pause and reflect on a few questions:\n",
    "* Why is our training score so much higher than testing?\n",
    "* How might we improve our scores with a linear model?\n",
    "    * Look at leverage and QQ-plots to identify points with high leverage and non-normal distributions. Feature selection and dimension reduction. \n",
    "* If we have a midling model, would you rely on the model weights to drive insights? Think about Anscombe's Quartet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Ridge(alpha=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print summaries\n",
    "print(\"Score (Training):\")\n",
    "print(clf.score(X_train, y_train))\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Score (Test):\")\n",
    "print(clf.score(X_test, y_test))\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Intercept:\")\n",
    "print(clf.intercept_)\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Coefficients:\")\n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization imposes a penalty on having a lot of large weights. Notice how some of our ridge regression weights dropped to zero. Let's also look at the sum of our weights before and after regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Sum: {}\".format(round(sum(reg.coef_[0]), 1)))\n",
    "print(\"Regularized Sum: {}\".format(round(sum(clf.coef_[0]),1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's count the ways this was a bad model:\n",
    "1. We didn't test any assumptions of normality\n",
    "2. No checks of colinearity- can affect model performance\n",
    "3. Are the numeric features meaningful? i.e. Just because MSSubClass is a number (60, 70, 80...), doesn't mean math operations on those numbers are valid.\n",
    "4. We have a LOT of columns given how few rows of data we have, and usually want 100 rows per column. We're about 21x short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE according to Kaggle method (RMSE between log-pred and log-act)\n",
    "## Prep a dataframe to accept results\n",
    "y_score = copy.deepcopy(y_test)\n",
    "## Run predictions on test set\n",
    "y_score['Pred_clf'] = clf.predict(X_test)\n",
    "## Score RMSE\n",
    "RMSE_clf = getRMSE(data=y_score, target='SalePrice', prediction='Pred_clf')\n",
    "Percentile_clf = round(stats.percentileofscore(leaderboard.Score, RMSE_clf),1)\n",
    "print(\"RMSE: {}\".format(round(RMSE_clf, 5)))\n",
    "print(\"Percentile on Leaderboard: {}%\".format(Percentile_clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "## Feature Engineering\n",
    "We'll start by exporting data out to our IDEAR tool. This script streamlines common exploratory data analyses to help us find interesting patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExportForIDEAR(data, target, filepath='idear_data.csv', verbose=False):\n",
    "    data_cat = list(data.select_dtypes(include='object'))\n",
    "    data_num = list(data.select_dtypes(exclude='object'))\n",
    "#     data_num = [x for x in set(data_num) - set(target)]\n",
    "    out_dict = {'CategoricalColumns': data_cat,\n",
    "                'DataFilePath': filepath,\n",
    "                'NumericalColumns': data_num,\n",
    "                'Target': target}\n",
    "    # Save yaml definition to file\n",
    "    with open('IDEAR/data.yaml', 'w') as outfile:\n",
    "        yaml.dump(out_dict, outfile, default_flow_style=False)\n",
    "    \n",
    "    # Write dataset to csv\n",
    "    data.to_csv('IDEAR/'+filepath)\n",
    "    \n",
    "    # And prep output version for inspection\n",
    "    if verbose:\n",
    "        yaml_out = yaml.dump(out_dict, default_flow_style=False)\n",
    "        return(yaml_out)\n",
    "\n",
    "ExportForIDEAR(home_data, target='SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "From our IDEAR analysis, we see an obvious need to clean the data. Let's start there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataframe to capture our transformations\n",
    "engineered_Data = copy.deepcopy(home_data)\n",
    "## Remove the ID column, this is an index with no predictive power\n",
    "engineered_Data.drop(columns='Id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA Filler\n",
    "def FillNACols(data, colname, value):\n",
    "    data[colname][data[colname].isna()] = value\n",
    "    return(data)\n",
    "\n",
    "# Inspect NA counts\n",
    "## Get the counts of NAs\n",
    "na_counts = count_NA(engineered_Data)\n",
    "## Print details about each column with NAs\n",
    "for idx in na_counts['index']:\n",
    "    print('Feature: {}'.format(idx))\n",
    "    print('Number of NAs: {}'.format(na_counts.NACount[na_counts['index']==idx].item()))\n",
    "    print(engineered_Data[idx].unique())\n",
    "    print('==============================================')\n",
    "\n",
    "# Fill values\n",
    "## Simple fills\n",
    "engineered_Data = FillNACols(engineered_Data, 'Alley', 'None')\n",
    "engineered_Data = FillNACols(engineered_Data, 'PoolQC', 'None')\n",
    "engineered_Data = FillNACols(engineered_Data, 'MiscFeature', 'None')\n",
    "engineered_Data = FillNACols(engineered_Data, 'Fence', 'None')\n",
    "engineered_Data = FillNACols(engineered_Data, 'FireplaceQu', 'None')\n",
    "engineered_Data = FillNACols(engineered_Data, 'GarageFinish', 'None')\n",
    "engineered_Data = FillNACols(engineered_Data, 'GarageCond', 'None')\n",
    "engineered_Data = FillNACols(engineered_Data, 'GarageQual', 'None')\n",
    "engineered_Data = FillNACols(engineered_Data, 'GarageType', 'None')\n",
    "engineered_Data = FillNACols(engineered_Data, 'BsmtExposure', 'None')\n",
    "engineered_Data = FillNACols(engineered_Data, 'BsmtQual', 'None')\n",
    "engineered_Data = FillNACols(engineered_Data, 'BsmtCond', 'None')\n",
    "engineered_Data = FillNACols(engineered_Data, 'BsmtFinType1', 'None')\n",
    "engineered_Data = FillNACols(engineered_Data, 'BsmtFinType2', 'None')\n",
    "\n",
    "## Remove Rows\n",
    "engineered_Data = engineered_Data = engineered_Data[pd.notna(engineered_Data['MasVnrType'])]\n",
    "engineered_Data = engineered_Data = engineered_Data[pd.notna(engineered_Data['Electrical'])]\n",
    "\n",
    "## Remove Columns\n",
    "cols_to_drop = ['GarageYrBlt', 'LotFrontage']\n",
    "try:\n",
    "    engineered_Data.drop(columns=cols_to_drop, inplace=True)\n",
    "except:\n",
    "    print(\"Columns already dropped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply power transformations to non-normal distributions, getting them closer to Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power transforms\n",
    "def log_transform(data):\n",
    "    data = np.log(data)\n",
    "    return(data)\n",
    "\n",
    "def reverse_log_transform(data):\n",
    "    data = np.exp(data)\n",
    "    return(data)\n",
    "    \n",
    "# List columns to transform\n",
    "cols_to_pt = ['SalePrice',\n",
    "              '1stFlrSF',\n",
    "              '2ndFlrSF',\n",
    "              'GrLivArea',\n",
    "              'LotArea']\n",
    "\n",
    "## Add a little noise before transforming\n",
    "engineered_Data[cols_to_pt] = engineered_Data[cols_to_pt] + 0.01\n",
    "engineered_Data[cols_to_pt] = log_transform(engineered_Data[cols_to_pt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some categories have a lot of levels, many of which have few samples. We will want to clean these up as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what our cleanup does to our model. \n",
    "Note that we reduce RMSE by 0.01, or ~6%, which is good enough for a 9% jump on the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_score(RMSE_score, leaderboard):\n",
    "    Percentile = round(stats.percentileofscore(leaderboard.Score, RMSE_score), 1)\n",
    "    Rank = float(sum([x < RMSE_score for x in leaderboard.Score]))\n",
    "    print(\"RMSE: {}\".format(round(RMSE_score, 5)))\n",
    "    print(\"Percentile on Leaderboard: {}%\".format(Percentile))\n",
    "    print(\"Leaderboard Rank: {} of {}\".format(int(Rank), len(leaderboard)))\n",
    "\n",
    "# Function to evaluate feature performance on regularized LM\n",
    "def evaluate_model(data):\n",
    "    # Split our data columns into features and target tables\n",
    "    X = data.drop(columns=['SalePrice'])\n",
    "    y = data[['SalePrice']].astype('float')\n",
    "    # Apply transformations\n",
    "    X_lm = one_hot_mixed_df(X)\n",
    "    # Then split the rows into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_lm, y, test_size=0.3, random_state=1984)\n",
    "\n",
    "    clf = Ridge(alpha=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate RMSE according to Kaggle method (RMSE between log-pred and log-act)\n",
    "    ## Prep a dataframe to accept results\n",
    "    y_score = copy.deepcopy(y_test)\n",
    "    ## Run predictions on test set\n",
    "    y_score['Pred_clf'] = clf.predict(X_test)\n",
    "    ## Inverse log to get back to real prices\n",
    "    y_score = reverse_log_transform(y_score)\n",
    "\n",
    "    ## Score RMSE\n",
    "    RMSE_clf = getRMSE(data=y_score, target='SalePrice', prediction='Pred_clf')\n",
    "#     Percentile_clf = round(stats.percentileofscore(leaderboard.Score, RMSE_clf),1)\n",
    "#     print(\"RMSE: {}\".format(round(RMSE_clf, 5)))\n",
    "#     print(\"Percentile on Leaderboard: {}%\".format(Percentile_clf))\n",
    "    kaggle_score(RMSE_clf, leaderboard)\n",
    "\n",
    "# Evaluate current configuration \n",
    "evaluate_model(engineered_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Features\n",
    "\n",
    "All we've done so far is janitorial work. Feature engineering is also the step where we start to incorporate subject matter expertise to synthesize inputs that add significant performance. Some ideas include:\n",
    "* Adjust the lot size per neighborhood. Some neighborhoods are more spacious, so maybe the relative size is more important than absolute square feet?\n",
    "* From PCA, we see that some homes show tell-tale signs of being rentals with many people (e.g. multiple kitchens, high count of beds and baths). Try setting a flag for such properties.\n",
    "* When house shopping, one strategy for home valuation is to study the multiplier between tax assessments and sale prices of homes in a neighborhood, then applying this scaling to the tax assessment of a property. How can we do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taxes\n",
    "Let's visit the Ames, Iowa city assessor: [LINK](https://www.cityofames.org/government/departments-divisions-a-h/city-assessor)\n",
    "\n",
    "Under \"Reports\", it looks like we can find what we need. Luckily, this report is already downloaded to our data folder. What's in there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_Data = pd.read_excel('../Data/AmesRealEstateData.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename some columns to match our dataset\n",
    "tax_Data.rename(index=str, columns={\"GLA\": \"GrLivArea\", \"YrBuilt\": \"YearBuilt\"}, inplace=True)\n",
    "tax_Data.head()\n",
    "\n",
    "# Remove incomplete tax records\n",
    "tax_Data = tax_Data[(tax_Data.GrLivArea.notna()) & (tax_Data.LotArea.notna()) & (tax_Data.YearBuilt.notna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  |  \".join(list(tax_Data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, our training data doesn't list addresses or lot ids of any kind. We need to get creative to combine this data. Instead of a clear \"key\" (i.e. address) to join our data, we need to generate a synthetic key based on identifying features. \n",
    "\n",
    "Inspecting the overlap of columns below, we see many shared features that are candidates for joining. Bear in mind, however, that some features (like Roofing material) are subject to change over time and may not be as reliable as lot size, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List overlapping columns\n",
    "print(set(list(tax_Data)) & set(list(engineered_Data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we see that many records match outright. While this is pretty good, we need significantly more coverage in order to use this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_key = ['GrLivArea', 'YearBuilt', 'LotArea', 'Neighborhood']\n",
    "\n",
    "# Try an inner join to see how many records line up\n",
    "print(\"Sample records in dataset: {}\".format(len(engineered_Data)))\n",
    "\n",
    "record_overlap = home_data.merge(tax_Data, on=syn_key, how='inner').drop_duplicates()\n",
    "print(\"Overlapping records: {}\".format(len(record_overlap.Id.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate neighborhood naming\n",
    "print(\"Neighborhoods in home data missing from tax data\")\n",
    "print(list(set(home_data.Neighborhood.unique()) - set(tax_Data.Neighborhood.unique())))\n",
    "\n",
    "print(\"Home Data:\")\n",
    "print(list(set(home_data.Neighborhood.unique())))\n",
    "print(\"Tax Data:\")\n",
    "print(list(set(tax_Data.Neighborhood.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like we need to clean up. Drop whitespace and special characters\n",
    "home_data['Neighborhood'] = home_data['Neighborhood'].str.strip()\n",
    "tax_Data['Neighborhood'] = tax_Data['Neighborhood'].str.strip()\n",
    "tax_Data['Neighborhood'] = tax_Data['Neighborhood'].str.replace('\\W', '')\n",
    "\n",
    "tax_Data.loc[tax_Data.Neighborhood=='Bluestm', 'Neighborhood'] = 'Blueste'\n",
    "tax_Data.loc[tax_Data.Neighborhood=='NoPkVil', 'Neighborhood'] = 'NPkVill'\n",
    "tax_Data.loc[tax_Data.Neighborhood=='Stonebr', 'Neighborhood'] = 'StoneBr'\n",
    "tax_Data.loc[tax_Data.Neighborhood=='NRidgHt', 'Neighborhood'] = 'NridgHt'\n",
    "tax_Data.loc[tax_Data.Neighborhood.isna(), 'Neighborhood'] = 'None'\n",
    "print(\"Remaining Neighborhood Mismatches:\")\n",
    "print(print(list(set(home_data.Neighborhood.unique()) - set(tax_Data.Neighborhood.unique()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve matches, we'll use a distance-based measure to match home records to their closest tax record counterpart. This will maintain perfect matches (i.e. distance == 0), but will accomodate records with imperfect matches. In addition to compensating for changes in a home's definition, it also effectively matches the record to the nearest comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def match_score(taxdf, targetRecord, keys):\n",
    "    taxdf = taxdf.loc[:, keys + ['Prop_Addr', 'TtlVal_AsrYr']]\n",
    "    targetRecord = targetRecord.loc[:, keys]\n",
    "    \n",
    "    # List numeric columns\n",
    "    num_keys = list(targetRecord.loc[:, keys].select_dtypes(include='number'))\n",
    "    # List object columns\n",
    "    obj_keys = list(targetRecord.loc[:, keys].select_dtypes(include='object'))\n",
    "    \n",
    "    # Measure errors\n",
    "    for key in num_keys:\n",
    "        # Measure error\n",
    "        taxdf['Error{}'.format(key)] = np.abs(taxdf[key] - targetRecord[key].item())\n",
    "        # Scale\n",
    "        scaler = StandardScaler(with_mean=False)\n",
    "        taxdf['Error{}'.format(key)] = scaler.fit_transform(np.array(taxdf['Error{}'.format(key)]).reshape(-1,1))\n",
    "\n",
    "    for key in obj_keys:\n",
    "        # Measure error- 0 if match, 1 if not matched\n",
    "        taxdf['Error{}'.format(key)] = taxdf[key] != targetRecord[key].item()\n",
    "        taxdf['Error{}'.format(key)] = taxdf['Error{}'.format(key)].astype('int') * 5\n",
    "        \n",
    "    # Distance measure\n",
    "    taxdf['Similarity'] = taxdf[taxdf.columns[pd.Series(taxdf.columns).str.startswith('Error')]].sum(axis=1)\n",
    "    \n",
    "    # Re-arrange\n",
    "    taxdf = taxdf.sort_values('Similarity', ascending=True)\n",
    "    return(taxdf.iloc[0,:]['TtlVal_AsrYr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of tax assessments using our matching function\n",
    "tax_assessments = [match_score(tax_Data, engineered_Data.iloc[x:x+1, :], syn_key) for x in range(0, len(engineered_Data))]\n",
    "# Join the list to our dataset\n",
    "engineered_Data['tax_assessment'] = tax_assessments\n",
    "engineered_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reflect on what we've done here- we've taken an anonymized data set, and de-anonymized it. In this case, the \"anonymous\" data used in the competition is publicly available and was only witheld for competition purposes, but this raises a couple of interesting questions:\n",
    "* Even if we anonymize our data, can it be reconstituted? Is that a problem?\n",
    "* In the case of the competition, we *could* now simply join published SalePrices and have *perfect* predictions. We'd win the competition, but how predictive is this model? This is called target leakage, and it's a serious issue because it's not always so obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tax Multiplier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily place a non-logged saleprice column\n",
    "engineered_Data['SalePrice_NoLog'] = np.exp(engineered_Data['SalePrice'])\n",
    "# Calculate neighborhood means\n",
    "nbd_tax_mult = engineered_Data.groupby('Neighborhood').agg({'SalePrice_NoLog': 'mean', 'tax_assessment': 'mean'})\n",
    "# Calculate ratios\n",
    "nbd_tax_mult['tax_mult'] = nbd_tax_mult['SalePrice_NoLog'] / nbd_tax_mult['tax_assessment']\n",
    "nbd_tax_mult.drop(columns=['SalePrice_NoLog', 'tax_assessment'], inplace=True)\n",
    "\n",
    "# Join back into engineered_Data\n",
    "engineered_Data = engineered_Data.merge(nbd_tax_mult, on='Neighborhood', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scaled tax assessment\n",
    "engineered_Data['scaled_tax_assessment'] = engineered_Data['tax_assessment'] * engineered_Data['tax_mult']\n",
    "\n",
    "# Clean up the temp columns\n",
    "engineered_Data.drop(columns=['SalePrice_NoLog', 'tax_mult'], inplace=True)\n",
    "\n",
    "engineered_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative Lot Size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean lot area by neighborhood\n",
    "lot_mult = engineered_Data.groupby('Neighborhood').agg({'LotArea': 'mean'})\n",
    "lot_mult.rename(index=str, columns={\"LotArea\": \"MeanNBDLotArea\"}, inplace=True)\n",
    "# Join with dataset\n",
    "engineered_Data = engineered_Data.merge(lot_mult, on='Neighborhood', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate individual lot size ratios\n",
    "engineered_Data['RelativeLotArea'] = engineered_Data['LotArea'] / engineered_Data['MeanNBDLotArea']\n",
    "\n",
    "# Clean up the temp columns\n",
    "engineered_Data.drop(columns=['MeanNBDLotArea'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "Whereas linear models represent patterns in our data with straight lines/planes, machine learning methods can fit non-linear shapes to our data (see example below). As we travel down this path, we trade interpretability for performance. Some methods are more interpretable (decision trees), while others discard interpretability altogether for maximum performance (neural networks).\n",
    "\n",
    "Note that interpretability is relative. While it is nearly impossible to look at a neural network and make any sense of it, we can use sensitivity analysis to understand how it arrived at a recommendation.\n",
    "\n",
    "<img src=\"nonlinear_fits.gif\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data columns into features and target tables\n",
    "X = engineered_Data.drop(columns=['SalePrice'])\n",
    "y = engineered_Data[['SalePrice']].astype('float')\n",
    "# Apply transformations\n",
    "X_lm = one_hot_mixed_df(X)\n",
    "# Then split the rows into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lm, y, test_size=0.3, random_state=1984)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "run_cv = False\n",
    "\n",
    "if run_cv:\n",
    "    # Generate a subsample for quicker training\n",
    "    sub_idx = np.random.randint(0, len(y_train), int(len(y_train)*0.99))\n",
    "    X_train_cv = X_train.iloc[sub_idx,:]\n",
    "    y_train_cv = y_train.iloc[sub_idx,:]\n",
    "\n",
    "    xg_reg = xgb.XGBRegressor(objective ='reg:linear',\n",
    "                              learning_rate = 0.3,\n",
    "                              n_estimators = 25)\n",
    "\n",
    "    colsample_bytree = [0.3, 0.6, 0.8]\n",
    "    max_depth = [2, 4, 6, 8]\n",
    "    alpha = [1, 3, 8]\n",
    "    subsample = [0.3, 0.6, 0.8]\n",
    "    param_grid = dict(colsample_bytree=colsample_bytree,\n",
    "                      max_depth=max_depth,\n",
    "                      reg_alpha=alpha,\n",
    "                      subsample=subsample)\n",
    "\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=7)\n",
    "    grid_search = GridSearchCV(xg_reg, param_grid, scoring=\"neg_mean_squared_error\", n_jobs=-1, cv=kfold, verbose=1)\n",
    "    grid_result = grid_search.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "    print(\"Best Score (MSE): {} \\nParams: {}\".format(grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor(objective ='reg:linear',\n",
    "                          colsample_bytree = 0.8,\n",
    "                          learning_rate = 0.1,\n",
    "                          max_depth = 6,\n",
    "                          subsample=0.8,\n",
    "                          alpha = 1,\n",
    "                          n_estimators = 200,\n",
    "                         seed=555111)\n",
    "\n",
    "\n",
    "# Train model\n",
    "xgbData = xgb.DMatrix(X_train, label=y_train)\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "xg_reg.fit(X_train, y_train, eval_metric=\"rmse\", eval_set=eval_set, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will have a look at the learning curves for this model. The curves provide a lot of good feedback for model training.\n",
    "<img src=\"biasvariance.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = xg_reg.evals_result()\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.plot(x_axis, results['validation_0']['rmse'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['rmse'], label='Test')\n",
    "ax.legend()\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('XGBoost RMSE Learning Curves')\n",
    "plt.ylim(0,0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,35))\n",
    "xgb.plot_importance(xg_reg, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_plots(df, sample_fraction):\n",
    "    # Subsample plot data\n",
    "    df = df.sample(frac=sample_fraction, replace=False)\n",
    "    \n",
    "    # Residual plot\n",
    "    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    ax1.scatter('pred', 'residual', data=df, alpha=1, s=3)\n",
    "    ax1.axhline(y=0, c='k')\n",
    "    ax1.set_xlabel('Predicted')\n",
    "    ax1.set_ylabel('Residual')\n",
    "\n",
    "    # Pred-actual plot\n",
    "    ax2.scatter('y', 'pred', data=df, alpha=1, s=3)\n",
    "    ax2.plot((11,13), (11,13), ls=\"--\", c=\".3\")\n",
    "    ax2.set_xlabel('Actual')\n",
    "    ax2.set_ylabel('Predicted')\n",
    "    \n",
    "    \n",
    "    # Histogram of residuals\n",
    "    ax3.hist(df['residual'], bins = 20)\n",
    "    ax3.set_xlabel('Residual')\n",
    "    \n",
    "    # Distributions of salaries\n",
    "    sns.kdeplot(df['pred'], ax=ax4)\n",
    "    sns.kdeplot(df['y'], ax=ax4)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Inverse transformation if using a transformed target variable\n",
    "y_measures = y_test\n",
    "# Prepare plot data\n",
    "plot_data = pd.DataFrame({'pred': preds.tolist()})\n",
    "plot_data['y'] = y_measures.reset_index()['SalePrice']\n",
    "\n",
    "\n",
    "# Calculate residuals\n",
    "plot_data['residual'] = plot_data['pred'] - plot_data['y']\n",
    "\n",
    "# calculate rmse\n",
    "mse = metrics.mean_squared_error(y_measures, preds)  \n",
    "rmse = np.sqrt(mse)\n",
    "print('RMSE: {}'.format(rmse))\n",
    "# calculate mae\n",
    "mae = metrics.mean_absolute_error(y_measures, preds)  \n",
    "print('MAE: {}'.format(mae))\n",
    "\n",
    "\n",
    "# Plot residuals    \n",
    "residual_plots(plot_data, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_score(RMSE_score=rmse, leaderboard=leaderboard)\n",
    "\n",
    "# Examine distribution of scores in first quartile\n",
    "leaderboard[leaderboard.Score < 0.3].Score.hist(bins = 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
